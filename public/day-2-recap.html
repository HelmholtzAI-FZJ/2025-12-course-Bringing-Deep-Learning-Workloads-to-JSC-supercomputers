<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Alexandre Strube // Sabrina Benassou // Ismail Khalfaoui">
  <title>Bringing Deep Learning Workloads to JSC supercomputers</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="./dist/reset.css">
  <link rel="stylesheet" href="./dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="./dist/theme/sky.css" id="theme">
  <style>
  .container{
    display: flex;
  }
  .col {
    flex: 1;
  }

  .slides {
      font-size: 0.75em;
  }
  .reveal ul {
      display: block;
  }
  .reveal ol {
      display: block;
  }

  img {
      max-height: 600px !important;
  }

  figcaption {
      font-size: 0.6em !important;
      font-style: italic !important;
  }

  .subtitle {
      font-style: italic !important;
  }

  .date {
      font-size: 0.75em !important;
  }


  body {
      font-family: "Arial", "sans-serif"
  }

  section {
      margin: 0;
  }

  .reveal .slides {
      margin: 0 1vmin;
  }
  .reveal h1,
  .reveal h2,
  .reveal h3,
  .reveal h4 {
      font-family: "Arial", "sans-serif";
      text-transform: Uppercase;
      color: #023d6b;
  }

  .reveal h1 {
      color: #023d6b;
      font-size: 250%;
  }


  .reveal h2 + h3 {
      text-transform: Unset;
      font-size: 80%;
  }

  .controls {
      visibility: hidden;
  }

  .reveal .progress {
      position: absolute;
      bottom: 1px;
  }

  .prompt {
      min-width: 0;
      width: 0;
      visibility: hidden;
  }

  div.dateauthor {
      padding-top: 4em;
      color: white;
  }

  div.prompt {
      width:0;
  }


  div#footer {
      position: fixed;
      bottom: 0;
      width: 100%;
      z-index: 10;
  font-size: 0.5em; font-weight: bold; padding: 0 1vmin; height: 20vmin; background: #fff}
  #footer h1 {
      position: absolute; 
      bottom: 3.2vmin; 
      display: block; 
      padding: 0 1em; 
      font-size: 1.7vmin;
      font-weight: bold;
      text-transform: unset;
      color: #023d6b;
  }
  #footer h2 {display: block; padding: 0.em 1em 0;}

  img.fzjlogo {
      position: fixed;
      bottom: 0;
      right: 0;
      height: 24vmin; /* The height of the svg is about 3 times the height of the logo */
      margin-bottom: -3vmin; /* Baseline of logo should be about 5% of short side above edge. */
  }

  .rendered_html img, svg {
      max-height: 440px;
  }

  </style>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Bringing Deep Learning Workloads to JSC
supercomputers</h1>
  <p class="subtitle">Recap of Day 2</p>
  <p class="author">Alexandre Strube // Sabrina Benassou // Ismail
Khalfaoui</p>
  <p class="date">December 10th, 2025</p>
</section>

<section>
<section id="summary-of-day-2" class="title-slide slide level1">
<h1>Summary of Day 2</h1>

</section>
<section id="quick-recap" class="slide level2">
<h2>Quick recap</h2>
<p>The meeting focused on optimizing data loading and storage strategies
for machine learning projects on supercomputers, with detailed
explanations of file systems, data formats, and distributed training
techniques. Participants learned about efficient data handling methods,
including HDF5 storage and parallelized training approaches using
transformer language models and distributed data parallelism. The
session addressed technical challenges and troubleshooting issues during
the implementation of distributed training jobs, concluding with
discussions on various parallelization techniques and resources for
further learning.</p>
</section>
<section id="next-steps" class="slide level2">
<h2>Next steps</h2>
<p>Alexandre: Post yesterday’s recording on YouTube and share on Slack
Alexandre: Make meeting host available to Narimene for when he leaves
Participants: Clone the data loading repository from the link provided
in Slack Participants: Add ‘datasets’ to requirements.txt in SCVENF
template and run setup Participants: Download the WikiText dataset by
running the training script on login node before submitting to compute
nodes Participants: Comment out lines 76 and 135 in the training script
before downloading dataset Participants: Install torch_run_jsc package
Participants: Follow TODO instructions in to_distributed_training.py and
run_to_distributed_training.sbatch files Participants: Increase number
of nodes to 2 to test multi-node training with 8 GPUs Participants:
Update PyTorch to version 2.7 or 2.8 in requirements.txt for FSDP
functionality Participants: Remove PyTorch module loading from module.sh
when upgrading to PyTorch 2.7+ Participants: Complete and submit the
course feedback survey that will be sent after the session Narimene:
Consider adding coding examples for tensor parallelism and pipelining to
future course offerings</p>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="summary" class="title-slide slide level1">
<h1>Summary</h1>

</section>
<section id="optimizing-data-for-supercomputers" class="slide level2">
<h2>Optimizing Data for Supercomputers</h2>
<p>Alexandre introduced the session, noting that Sabrina would lead most
of the course due to his prior commitment. He explained the importance
of efficiently loading data onto supercomputers for machine learning,
emphasizing the need to keep GPUs busy and the significance of data
storage strategies. He highlighted the differences between classical and
deep learning in terms of data usage and described the supercomputer’s
file system, including project folders, scratch space, and data
directories. The session aimed to provide participants with insights
into optimizing data loading and storage for their projects.</p>
</section>
<section id="efficient-large-dataset-storage-strategies"
class="slide level2">
<h2>Efficient Large Dataset Storage Strategies</h2>
<p>Narimene explained how to efficiently load and store large datasets
for training, focusing on strategies for datasets smaller than 500GB
that can fit into RAM. She demonstrated methods for storing data in HDF
files and Apache Arrow format, addressing the system’s inode limitations
by showing how to create these files to avoid running out of available
inodes. The discussion included a practical example using the ImageNet
dataset, where images were transformed into binary format to ensure
compatibility with HDF and Arrow formats, and she provided instructions
for running example scripts to save and load the data.</p>
</section>
<section id="hdf5-and-distributed-training-overview"
class="slide level2">
<h2>HDF5 and Distributed Training Overview</h2>
<p>The meeting focused on clarifying file formats and systems, with
Narimene explaining that HDF5 is the preferred format due to its
performance and widespread use in the scientific community. They
discussed how to handle heterogeneous datasets using HDF5 groups and
binary formats for variable-sized data. The group then moved on to
parallelized training using a transformer language model with the
Wikitext dataset, with Alexandre reminding participants to clone the
necessary repository and install required datasets. The session
concluded with a review of training.py and
run_distributed_training.sbatch files, which participants were
instructed to run using SLURM batch commands.</p>
</section>
<section id="optimizing-gpu-utilization-in-training"
class="slide level2">
<h2>Optimizing GPU Utilization in Training</h2>
<p>The team discussed running distributed training jobs on the
supercomputer, focusing on GPU utilization and communication between
GPUs. Narimene explained that using only one GPU out of four available
was inefficient and demonstrated how to modify the SBATCH file to use
all GPUs. They discussed collective operations like all-reduce,
all-gather, and reduce-scatter for efficient communication between GPUs.
The team also addressed issues with data downloading and reservation
times for running jobs on the supercomputer.</p>
</section>
<section id="distributed-training-techniques-overview"
class="slide level2">
<h2>Distributed Training Techniques Overview</h2>
<p>The meeting covered the basics of distributed computing terminology
and distributed training techniques, with a focus on DDP (Distributed
Data Parallel). Narimene explained key concepts including word size,
rank, and locker rank, followed by a detailed explanation of DDP
implementation in PyTorch. The group discussed technical issues with
Python code execution and setup, with Alexandre providing assistance to
Agnieszka and others. The session concluded with a demonstration of
distributed training using 4 GPUs, showing improved GPU utilization and
reduced training time compared to single-GPU training.</p>
</section>
<section class="slide level2">

<p>The team discussed issues with running distributed training jobs and
code errors. Narimene helped Florianscheidl troubleshoot errors related
to torch run and environment setup, suggesting he clone the STV template
again and install dependencies correctly. Agnieszka faced an issue with
the sbatch file, which Narimene helped resolve by identifying and
removing an extra hashtag and replacing file names. Peter had forgotten
to remove “torch” before “safety room” in his code, which he fixed. The
team also clarified how to access LLV, which requires the same password
as Eudor.</p>
</section>
<section class="slide level2">

<p>The meeting covered several topics related to distributed training
techniques in deep learning. Narimene explained the concepts of Data
Parallelism (DDP) and Fully Sharded Data Parallelism (FSDP), including
how they work and when to use them. She also discussed other
parallelization techniques like tensor parallelism and pipelining. The
participants encountered some issues with installing the required
PyTorch version (2.6 or 2.7) for FSDP, but some were able to resolve
them. Narimene provided useful resources for further learning and
mentioned that a survey would be sent out to gather feedback on the
course.</p>
</section></section>
    </div>
  </div>

  <script src="./dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="./plugin/notes/notes.js"></script>
  <script src="./plugin/search/search.js"></script>
  <script src="./plugin/zoom/zoom.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // reveal.js plugins
        plugins: [
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
